{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student name and BC username**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collaborators names and BC usernames (max 2 collaborators)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font face = \"times\"><center>  EC1151 - Homework 12 - Due Dec 9th, Midnight</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"times\">\n",
    "# Instructions\n",
    "<br>\n",
    "\n",
    "+ You must fill out this Jupyter notebook and return it by e-mail to baisihad@bc.edu\n",
    "\n",
    "+ Double click on the cells to edit them\n",
    "\n",
    "+ For questions that need mathematical notation, try to use $\\LaTeX$ if you can.\n",
    "\n",
    "+ Sometimes, below a question that requires an answer there will be a cell with several `assert` statements. If you can run that cell without generating error, your answer has been validated and you are guaranteed the points.\n",
    "\n",
    "+ If you're stuck, try posting a question in the Piazza forum. Active contributors will be rewarded.\n",
    "\n",
    "+ Finally, this is <b>experimental</b>. I'd love to hear what you think about this homework format. You can e-mail me with your thoughts, or (preferrably) post in the Piazza forum (possibly anonymously to your peers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"times\">\n",
    "\n",
    "<font size=3>This homework is graded out of <font color=\"purple\"><b>100</b></font> points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll import all the libraries you may need ahead. (<font color=\"red\">Run the cell below!</font>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [-5,-4,5,8,11]\n",
    "np.mean(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1) You've been using a biased estimator [25 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that you have data on some random variable $X_1,\\cdots,X_n$ with $E[X_i] = \\mu$ and $Var[X_i] = \\sigma^2$.\n",
    "\n",
    "If you wanted to estimate the variance, you would probably compute\n",
    "\n",
    "$$\\hat{\\sigma}^2 = \\frac{1}{n}\\sum_i (X_i - \\hat{\\mu})^2$$\n",
    "\n",
    "would't you? Well, as it turns out, this is a *biased* estimator of $\\sigma^2$! \n",
    "\n",
    "An unbiased estimate would be this one:\n",
    "\n",
    "$$\\hat{\\sigma}_{unbiased}^2 = \\frac{1}{n-1}\\sum_i (X_i - \\hat{\\mu})^2$$\n",
    "\n",
    "The primary objective of this exercise is not so much understand why the denominator has to be corrected, but understand what the bias looks like, and also get you acquainted with some helpful properties of the variance.\n",
    "<br><br>\n",
    "\n",
    "<font size =1><b>Personal remark</b> sometimes, when people explain the origin of this different denominator, they'll often mumble something incomprehensible about *degrees of freedom*. I never really liked that explanation. The denominator looks the way it looks because that's the correction you need for the estimator to be unbiased - and that's it.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) For starters, prove that $Var(\\hat{\\mu}) = E[(\\hat{\\mu} - \\mu)^2] = \\frac{\\sigma^2}{n}$. (We've seen this proof a few times.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q1a",
     "locked": false,
     "points": 5,
     "solution": true
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Now prove this very useful identity: for any random variable $Z$, \n",
    "\n",
    "$$Var[Z] = E[Z^2] - E[Z]^2$$\n",
    "<br>\n",
    "<font size=1>Hint: Recall the definition of variance, expand the square, and then use several properties of expectations.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q1b",
     "locked": false,
     "points": 5,
     "solution": true
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Use what you learned in parts $a$ and $b$ to show that\n",
    "\n",
    "+ $E[X_i^2] = \\mu^2 + \\sigma^2$\n",
    "+ $E[\\hat{\\mu}^2] = \\mu^2 + \\frac{\\sigma^2}{n}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q1c",
     "locked": false,
     "points": 2,
     "solution": true
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Prove that the identity you showed for the theoretical variance also holds for the empirical variance.\n",
    "\n",
    "$$\\frac{1}{n}\\sum_{i} (X_i - \\hat{\\mu})^2 = \\frac{1}{n}\\sum_i X_i^2 - \\hat{\\mu}^2$$\n",
    "\n",
    "<font size=1>(I hope you can see the parallel between parts b and d!)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q1d",
     "locked": false,
     "points": 3,
     "solution": true
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Now take expectations of $\\hat{\\sigma}^2$. If $\\hat{\\sigma}^2$ were unbiased, we'd have that $E[\\hat{\\sigma}^2] = \\sigma^2$. As you'll see, this is not true, and we end up with $$E[\\hat{\\sigma}^2] = E \\left[\\frac{1}{n}\\sum_{i} (X_i - \\hat{\\mu})^2 \\right] = \\sigma^2 + \\text{bias}$$\n",
    "\n",
    "What is the value of bias?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q1e",
     "locked": false,
     "points": 8,
     "solution": true
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) Asymptotically*, what happens to the bias?\n",
    "\n",
    "<font size=1>The word <i>asymptotically</i> means <i>as $n\\rightarrow\\infty$</i>. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q1f",
     "locked": false,
     "points": 2,
     "solution": true
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2) Ommitted variable bias [15 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scatterplots below contain information about three variables $(X, Y, Z)$ (darker blobs indicate higher levels of Z).\n",
    "\n",
    "Visually inspect the scatterplots and answer the following:\n",
    "\n",
    "a) Classify the six figures below into those where the **causal** effect of $X$ on $Y$ positive, negative and zero.\n",
    "\n",
    "b) For which cases a regression of $Y$ on $X$ only would still produce unbiased estimates of the causal effect? For each of the other cases: will there be a positive bias (i.e., will you overestimate the effect) or negative bias (underestimate the effect)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "<img src=\"fig1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q2",
     "locked": false,
     "points": 15,
     "solution": true
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3) Cobb-Douglas Production Functions [60 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Economics, one extremely common way to model firm productions is to use a <i>Cobb-Douglas</i> production function. That is:\n",
    "\n",
    "$$Y = AK^\\alpha L^{\\beta} \\qquad \\alpha, \\beta \\ \\text{ in } \\ (0,1)$$\n",
    "\n",
    "Where $Y$ represents output, $L$ is labor (i.e., how many man-hours were spent in production), and $K$ is the value of capital (i.e., machinery, infrastructure, etc). The term $A$ is called *Total Factor Productivity* (TFP). It's usually assumed to be the same across all firms, and conveniently lumps together all other factors that influence production but are not captured by $L$ or $K$ (like geography, law, technological advancement, etc). You can read more about Cobb-Douglas production functions <a href=\"https://en.wikipedia.org/wiki/Cobb%E2%80%93Douglas_production_function\">here</a>. \n",
    "\n",
    "We'd like to estimate the value of the coefficients $\\alpha$ and $\\beta$ using linear regression. If we take (natural) logs of the expression above and assume that it will hold approximately in the data, we have that:\n",
    "\n",
    "$$\\log Y = \\log A + \\alpha\\log K + \\beta\\log L + \\epsilon$$\n",
    "\n",
    "The file `cobbdouglas.csv` contains actual data on $\\log Y, \\log K, \\log L$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>logProduction</th>\n",
       "      <th>logCapital</th>\n",
       "      <th>logLabor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.605170</td>\n",
       "      <td>4.605170</td>\n",
       "      <td>4.605170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.615121</td>\n",
       "      <td>4.672829</td>\n",
       "      <td>4.653960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.718499</td>\n",
       "      <td>4.736198</td>\n",
       "      <td>4.700480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.804021</td>\n",
       "      <td>4.804021</td>\n",
       "      <td>4.770685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.820282</td>\n",
       "      <td>4.875197</td>\n",
       "      <td>4.812184</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   logProduction  logCapital  logLabor\n",
       "0       4.605170    4.605170  4.605170\n",
       "1       4.615121    4.672829  4.653960\n",
       "2       4.718499    4.736198  4.700480\n",
       "3       4.804021    4.804021  4.770685\n",
       "4       4.820282    4.875197  4.812184"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"cobbdouglas.csv\") # File must be in same folder as this HW!\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Go back to Day36 (or read the documentation <a href=\"http://statsmodels.sourceforge.net/devel/example_formulas.html\">here</a> and use the `statsmodels.formula.api` module to fit a linear regression, and print a summary table containing the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "q3a",
     "locked": false,
     "points": 5,
     "solution": true
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) What are the estimated values of $\\alpha$, $\\beta$ and $\\log A$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q3b",
     "locked": false,
     "points": 5,
     "solution": true
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Find out the average value of $logCapital$ and $logLabor$, and compute your estimate for $logProduction$ using these average values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "q3c",
     "locked": false,
     "points": 5,
     "solution": true
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) According to the table are $\\log A$, $\\alpha$ and $\\beta$ statistically significantly different from zero?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q3d",
     "locked": false,
     "points": 5,
     "solution": true
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Suppose the table weren't as helpful, and we had to figure out all by ourselves the t-statistics, the p-values and the $95%$ confidence intervals associated with the hypotheses:\n",
    "\n",
    "$$H_0: \\alpha = 0 \\qquad H_1: \\alpha \\neq 0$$\n",
    "\n",
    "$$H_0: \\beta = 0 \\qquad H_1: \\beta \\neq 0$$\n",
    "\n",
    "As you'll learn in Econometrics, if your estimates are unbiased,\n",
    "\n",
    "$$\\hat{\\alpha} \\sim^a Normal(\\alpha, \\ \\sigma_\\alpha^2) \\qquad \\hat{\\beta} \\sim^a Normal(\\beta,  \\ \\sigma_\\beta^2) $$\n",
    "\n",
    "Under the null, each of those becomes\n",
    "\n",
    "$$\\hat{\\alpha} \\sim^a Normal(0, \\ \\sigma_\\alpha^2) \\qquad \\hat{\\beta} \\sim^a Normal(0,  \\ \\sigma_\\beta^2) $$\n",
    "\n",
    "That is, by the CLT, the estimates are Normally distributed around the their true values with some appropriate variance $\\sigma_\\alpha^2, \\sigma_\\beta^2$ -- we won't bother with the actual expression for these numbers right now. You'll have plenty of fun doing that next semester (<font size=4>ðŸŽ‰</font>).\n",
    "\n",
    "Suppose we knew $\\sigma_{\\alpha} = \\sigma_{\\beta} = .1$ (say). \n",
    "\n",
    "Find the p-values associated with $\\hat{\\alpha}$ and $\\hat{\\beta}$ using Python.\n",
    "\n",
    "a) $P(\\hat{\\alpha} < -0.2331) + P(\\hat{\\alpha} > 0.2331)$\n",
    "\n",
    "b) $P(\\hat{\\beta} < -0.8073) + P(\\hat{\\beta} > 0.8073)$\n",
    "<br><br>\n",
    "<font size=1>Do you understand why this is the p-value? Make sure you do!</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q4e",
     "locked": false,
     "points": 5,
     "solution": true
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) Once again suppose that we *knew* $\\sigma_{\\alpha} = \\sigma_{\\beta} = .1$.\n",
    "\n",
    "Our estimates are approximately Normally distributed. So if we subtract their means and divide by their standard deviations, the resulting object is Normally distributed with mean zero and variance one. \n",
    "\n",
    "$$\\frac{\\hat{\\alpha} - \\alpha}{\\sigma_{\\alpha}} \\sim^a N(0,1) \\qquad \\text{and} \\qquad \\frac{\\hat{\\beta} - \\beta}{\\sigma_{\\beta}} \\sim^a N(0,1)$$\n",
    "\n",
    "The table below contains the values for the cumulative probability distribution of a standard normal distribution. How could you use it to recompute the p-values? \n",
    "\n",
    "<img src=\"ztable.png\" width = 400>\n",
    "\n",
    "You should get roughly the same numbers as in part e (up to numerical precision).\n",
    "\n",
    "\n",
    "<font size=1>\n",
    "See page 84 of the textbook if you are having trouble here\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q4f",
     "locked": false,
     "points": 5,
     "solution": true
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g) Still assuming that $\\sigma_{\\alpha} = \\sigma_{\\beta} = 0.1$, would you reject each null hypothesis at a significance level of $1\\%$? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q4g",
     "locked": false,
     "points": 5,
     "solution": true
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h) Since we don't know $\\sigma_\\alpha$ or $\\sigma_\\beta$, we have to estimate it. Luckily, that has already been done for us: the column **std err** refers to the *standard error*, and standard errors are *estimates of the $\\sigma_\\alpha$ and $\\sigma_\\beta$*. Looking at the table, we have that \n",
    "\n",
    "$$\\hat{\\sigma}_{\\alpha} =  0.064 \\qquad \\hat{\\sigma}_{\\beta} = 0.145$$\n",
    "\n",
    "Now, <u>when $n$ is large</u>, even if we put $\\hat{\\sigma}^2$ in place of the true $\\sigma^2$, the estimates will still be roughly Normal.\n",
    "\n",
    "Compute the p-values in Python again, now using these estimates in place of 0.1. Would you reject the null hypotheses?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "q4h",
     "locked": false,
     "points": 2,
     "solution": true
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i) How would you recompute the p-values using the standard Normal table in part f?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q4i",
     "locked": false,
     "points": 3,
     "solution": true
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "j) One common economic assumption is that the production function has *constant returns to scale*. That is, if you double the inputs, the output should be doubled as well. Let's see what this means algebraically.\n",
    "\n",
    "$$Y = AL^{\\alpha}K^{\\beta}$$\n",
    "\n",
    "If we double the inputs...\n",
    "\n",
    "$$2Y = A(2L)^{\\alpha}(2K)^{\\beta} = 2^{\\alpha+\\beta} AL^{\\alpha}K^{\\beta} = 2Y$$\n",
    "\n",
    "...all these equalities will hold true *if* $\\alpha + \\beta = 1$. So that's our condition.\n",
    "\n",
    "Now, we're interested in testing whether the condition $\\alpha + \\beta = 1$ is matched in the data. Looking at the estimates $\\hat{\\alpha}, \\hat{\\beta}$ on the summary table, we see that their sum is indeed close to one, but let's be formalize this into a hypothesis test.\n",
    "\n",
    "$$H_0: \\alpha + \\beta = 1 \\qquad H_1: \\alpha + \\beta \\neq 1$$\n",
    "\n",
    "Ultimately, we want to know if our estimates $\\hat{\\alpha} + \\hat{\\beta}$ are so \"far away\" from 1 that we can reject $H_0$. As usual, \"far away\" means that an \"estimate as extreme as $\\hat{\\alpha} + \\hat{\\beta}$ would be very unlikely if $H_0$ were true\". In turn, to compute how likely things are, we first need to know the probability distribution of $\\hat{\\alpha} + \\hat{\\beta}$ under $H_0$. So let's start there.\n",
    "\n",
    "Using what you know about the Normal distribution: if\n",
    "\n",
    "$$\\hat{\\alpha} \\sim^a Normal(\\alpha, \\ \t0.064^2) \\qquad \\hat{\\beta} \\sim^a Normal(\\beta,  \\ 0.145^2) $$\n",
    "\n",
    "then what is the distribution of $\\hat{\\alpha} + \\hat{\\beta}$, *under the null hypothesis*?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q4j",
     "locked": false,
     "points": 5,
     "solution": true
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k) Use Python to find out the p-values associated with the null hypothesis above. Do you reject the null hypothesis that $\\alpha + \\beta = 1$ at a significance level of $5\\%$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "q4k",
     "locked": false,
     "points": 5,
     "solution": true
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "l) Standardize the statistic $\\hat{\\alpha} + \\hat{\\beta}$, then use the table in part f again to recompute the p-values. Your answer should be roughly the same as in part h."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q4l",
     "locked": false,
     "points": 5,
     "solution": true
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "m) At a 1% level, the numbers work out in such a way that you should definitely reject the hypothesis above. That is, our estimates of $\\hat{\\alpha} + \\hat{\\beta}$ are so far from 1 that it'd very unlikely to get a number that extreme if their mean were indeed 1.\n",
    "\n",
    "But at the 1% level, how high *could* $\\hat{\\alpha} + \\hat{\\beta}$ have been before we rejected the null hypothesis? Or how low? (And what do these two numbers represent?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q4m",
     "locked": false,
     "points": 5,
     "solution": true
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
