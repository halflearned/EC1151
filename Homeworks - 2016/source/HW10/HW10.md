# HW10 - Theory - Due April 19th (Wed)


### Q1 CLT review

Let's warm up by reminding ourselves of what the Central Limit Theorem (CLT) tells us. Throughout, we will suppose we have independent random variables $X_1, X_2, \cdots, X_n \sim Dist(\theta)$, where $Dist$ is some distribution (not necessarily Normal!), with some parameter $\theta$. 

1) In terms of $E[X_1]$ and $Var[X_1]$, what does the CLT tell us about the following random variable?

$$S = X_1 + \cdots + X_n$$

2) In terms of $E[X_1]$ and $Var[X_1]$, what does the CLT tell us about the following random variable?

$$W = \frac{X_1 + \cdots + X_n}{n}$$

3) Using your answer to part 2, explain why we can say that the Law of Large Numbers is a corollary of the Central Limit Theorem.


### Q2 Explaning MOM

[Open Ended] Explain in your own words:

1) What is an estimator?
2) What is the method of moments?
3) Why are estimators random variables?
4) Can we ever know true value of the parameters that generated our data?

Your answers can be as short or long as you find necessary -- within reason!

### Q3 Coin toss

I tossed one coin eight times, and here are the results I got, encoded as $1$ for Heads and $0$ for tails.

<centeR>

| <b>Toss </b> | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 |
|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
| <b>Value</b> | 1 | 0 | 1 |  1 | 1 | 1 | 0 | 0

</center>

Naturally, we can say that each value is 

$$X_i \sim Bernoulli(p)$$

for some value $p$.

1) Find the method of moments estimator for the parameter $p$ and compute an estimate using the data above.

2) What is the *approximate* distribution of the estimator? Give its name and parameters.

3) Suppose the coin was fair. *Using a probability table*, compute the probability of getting an estimate of $p_{MOM}$ at least as large as you got using the data above. Do *not* use Python.

<b>Remark</b> If you're confused by this question, remember that $p_{MOM}$ is a *random variable*, and the value you got is only one possible realization of that random variable. In other words, the question is saying this: 

> "Suppose you generated a *different* data set using the $Bernoulli(\frac{1}{2})$ distribution. Using this *different* data set, you'd get a *new* estimate. What is the probability that the *new* estimate is at least as large as the old one you got from *this* data set?

### Q4 Poisson bankruptcies

<font size=1>This will be another example of another method of moments estimator that happens to be an average.</font>


The Poisson distribution is used to model the number of rare and independent events. One such use is the number of bankruptcies in a region during a certain period (assuming they are independent). 

<br>Suppose you have the following data on bankrupcties:

<img src="poisson.png" width = 700>

These data were generated by 

$$X_1, \cdots, X_{10} \sim Poisson(\lambda)$$

1) Find the method of moments estimator for the parameter $\lambda$ and compute an estimate using the data above.

2) What is the approximate distribution of the estimator? Give its name and parameters.

3) Suppose the true value of $\lambda$ is 6. Compute $P(\lambda_{MOM} < 5)$ and $P(\lambda_{MOM} > 7)$ *using a probability table*.

### Q5 Normal MOM

The data below came from the $Normal(\mu, \sigma^2)$ distribution.

<center>

| <b>Individual</b> | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 
|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
| <b>Value</b> |  6.11 |  -4.19 |   0.86 | -4.52 |   1.1 |  0.52 |  -0.71 |  10.09 | 0.28 |  2.5 |
 
</center>

1) Find the method of moments estimator for the parameters $\mu$ and $\sigma^2$ and compute estimates using the data above.

2) Explain why the method of moments estimator $\mu_{MOM}$ is Normally distributed.

<b>Hint</b> HW9Q2.

3) Suppose that in reality $\mu = 3$ and $\sigma^2 = 4$. Compute the probability that $\mu_{MOM}$ is at *most* what you got in part 1.

4) [Optional - Not for credit] It will turn out that, for large $n$, the estimator $\sigma_{MOM}^2$ you computed above will also be Normally distributed. However, proving this will require more complicated math than we currently have access to. But why can't we use the CLT as we learned in class? What assumption(s) of the theorem are being violated?









